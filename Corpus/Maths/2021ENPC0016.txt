Cette thèse est motivée par l'étude des matrices de covariance, et s'articule naturellement en trois parties. Dans la première partie, nous étudions des modèles dynamiques liés aux matrices de covariance. Nous étudions plus précisément les systèmes d'équations différentielles stochastiques hérités de la dynamique des valeurs propres de processus matriciels appelés processus de Wishart et processus de Jacobi. Les solutions de ces systèmes sont respectivement les processus de [dollar]beta[dollar]-Wishart et de [dollar]beta[dollar]-Jacobi. Nous étendons les résultats connus d'existence et d'unicité de solutions à ces équations et caractérisons leur comportement en temps long. Dans la seconde partie, nous étudions à la lumière de résultats modernes de probabilités libres, en particulier sur la convolution libre rectangulaire additive, le comportement limite en grande dimension de la mesure empirique des particules constituant le processus de [dollar]beta[dollar]-Wishart, et établissons la commutativité entre la limite en temps long et la limite en grande dimension pour cette suite de processus à valeurs mesures. La troisième partie  porte sur l'étude de la stabilité de la procédure de rétro-propagation dans la phase d'apprentissage d'un réseau de neurones à propagation avant dont les couches sont de largeur variable. Cette étude se concentre sur la matrice Jacobienne du réseau, qui peut se décrire comme un long de produit de matrices, et dont les propriétés spectrales sont déterminantes pour la stabilité de la descente du gradient. Cela demande la définition d'une opération de convolution libre rectangulaire multiplicative. Nous proposons un algorithme efficace de calcul de la mesure du carré des valeurs singulières de cette matrice. Les conclusions de ce travail donnent des indicateurs permettant d'évaluer la stabilité d'un réseau de neurones, à vocation d'aide au design du réseau pour les praticiens.